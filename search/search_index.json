{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CausalBench Documentation This documentation serves as a guideline to CausalBench. As we are currently in beta, this documentation will be updated as the development progresses. Installation Details how to install causalbench, and prerequisites. Quickstart A tutorial python notebook file on how to: Install causalbench, Download a dataset, model, metric to create a scenario (or downloading a pre-set scenario) Upload a component Run a benchmark Upload results Learn the Basics Details the terminology, what a dataset, model, metric, context, benchmark, and more is. Terms and Conditions Read the terms to make CausalBench a safe and collaborative environment for open-source research . Demo Video","title":"Home"},{"location":"#welcome-to-causalbench-documentation","text":"This documentation serves as a guideline to CausalBench. As we are currently in beta, this documentation will be updated as the development progresses.","title":"Welcome to CausalBench Documentation"},{"location":"#installation","text":"Details how to install causalbench, and prerequisites.","title":"Installation"},{"location":"#quickstart","text":"A tutorial python notebook file on how to: Install causalbench, Download a dataset, model, metric to create a scenario (or downloading a pre-set scenario) Upload a component Run a benchmark Upload results","title":"Quickstart"},{"location":"#learn-the-basics","text":"Details the terminology, what a dataset, model, metric, context, benchmark, and more is.","title":"Learn the Basics"},{"location":"#terms-and-conditions","text":"Read the terms to make CausalBench a safe and collaborative environment for open-source research .","title":"Terms and Conditions"},{"location":"#demo-video","text":"","title":"Demo Video"},{"location":"basics/","text":"CausalBench 101 Dataset Description : Data and configuration files that describe the data in the data files. How to define a dataset A dataset is defined with a yaml configuration, and the data files. Any dataset configuration must inlcude a type, name, task(?), files defined. Files need to be configured with type , data type, file path in path , and any columns with their details. Optional fields are: headers , version_num , that defines the versions, url , source and description that define the dataset. type: dataset name: abalone source: UCI url: https://archive.ics.uci.edu/dataset/1/abalone description: Predict the age of abalone from physical measurements files: file1: type: csv data: dataframe path: abalone.mixed.numeric.csv headers: true columns: sex: header: Sex type: nominal data: integer labels: - 0 - 1 - 2 ... file2: type: csv data: graph path: causal_info_adjmat.csv headers: true columns: sex: header: Sex type: nominal data: integer ... Model Description : Algorithms written in Python that take in a dataset and execute a particular model. Function : Produce outputs based on the tasks and models. How to define a model A model is defined with a yaml configuration, alongside a python script that the model is executed on. A model must have a type, name, task and its path defined. Optional fields are: version_num , that defines the versions, url , source and description that define the model. type: model name: VAR-LiNGAM source: GitHub url: https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle description: Discovery of non-gaussian linear causal models task: discovery.temporal path: varlingam.py version_num: 1 Metric Description : Python implementations of metric calculations. Function : Take in the outputs provided by the model and output a numerical value, based on its configuration. How to define a metric type: metric name: accuracy_static source: NA url: NA description: Accuracy metric for static causal graph. task: discovery.static path: accuracy_static.py Context Description : A configuration that encapsulates a selected set of multiple datasets, models, metrics and hyperparameters. Function : Defined by users, contexts represent experiments carried out on a selection of components. Contexts are defined on user level, and all of the dataset-model tuple combinations are created and executed on the system level. How to define a context A context is created with a yaml notation. Any context should have a type, name, task(?), and at least one dataset, model and metric defined. name: Context Abhinav 1 task: discovery.static description: July experiment set datasets: - id: 14 version: 1 - id: 15 version: 1 models: - id: 22 version: 1 - id: 17 version: 1 metrics: - id: 12 version: 1 - id: 12 version: 2 Task Description : Logical definition of Causal/Machine Learning tasks. Function : Handles configurations between dataset/model/metric components through python functions. How to define a task A task takes in two files: a .yaml file that defines the type, name, path and the class name of a task, and a .py file that defines the task and its execution. The yaml file should only include the type , name , path , and the class fields. type: task name: discovery.static path: discovery.static.py class_name: DiscoveryStatic The python file should include any input and output functions for any dataset -> model, model -> metric, dataset -> metric relations, and the helper functions to facilitate the task. discovery.static.py and discovery.temporal.py files under the /task folder provides examples on construction of these functions. Benchmark Description : Benchmark defines run results of a number of executed contexts. On top of the executed context structure, benchmark includes run-configuration specific information like the system configuration, GPU-CPU resource profiling.","title":"Learn the Basics"},{"location":"basics/#causalbench-101","text":"","title":"CausalBench 101"},{"location":"basics/#dataset","text":"Description : Data and configuration files that describe the data in the data files.","title":"Dataset"},{"location":"basics/#how-to-define-a-dataset","text":"A dataset is defined with a yaml configuration, and the data files. Any dataset configuration must inlcude a type, name, task(?), files defined. Files need to be configured with type , data type, file path in path , and any columns with their details. Optional fields are: headers , version_num , that defines the versions, url , source and description that define the dataset. type: dataset name: abalone source: UCI url: https://archive.ics.uci.edu/dataset/1/abalone description: Predict the age of abalone from physical measurements files: file1: type: csv data: dataframe path: abalone.mixed.numeric.csv headers: true columns: sex: header: Sex type: nominal data: integer labels: - 0 - 1 - 2 ... file2: type: csv data: graph path: causal_info_adjmat.csv headers: true columns: sex: header: Sex type: nominal data: integer ...","title":"How to define a dataset"},{"location":"basics/#model","text":"Description : Algorithms written in Python that take in a dataset and execute a particular model. Function : Produce outputs based on the tasks and models.","title":"Model"},{"location":"basics/#how-to-define-a-model","text":"A model is defined with a yaml configuration, alongside a python script that the model is executed on. A model must have a type, name, task and its path defined. Optional fields are: version_num , that defines the versions, url , source and description that define the model. type: model name: VAR-LiNGAM source: GitHub url: https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle description: Discovery of non-gaussian linear causal models task: discovery.temporal path: varlingam.py version_num: 1","title":"How to define a model"},{"location":"basics/#metric","text":"Description : Python implementations of metric calculations. Function : Take in the outputs provided by the model and output a numerical value, based on its configuration.","title":"Metric"},{"location":"basics/#how-to-define-a-metric","text":"type: metric name: accuracy_static source: NA url: NA description: Accuracy metric for static causal graph. task: discovery.static path: accuracy_static.py","title":"How to define a metric"},{"location":"basics/#context","text":"Description : A configuration that encapsulates a selected set of multiple datasets, models, metrics and hyperparameters. Function : Defined by users, contexts represent experiments carried out on a selection of components. Contexts are defined on user level, and all of the dataset-model tuple combinations are created and executed on the system level.","title":"Context"},{"location":"basics/#how-to-define-a-context","text":"A context is created with a yaml notation. Any context should have a type, name, task(?), and at least one dataset, model and metric defined. name: Context Abhinav 1 task: discovery.static description: July experiment set datasets: - id: 14 version: 1 - id: 15 version: 1 models: - id: 22 version: 1 - id: 17 version: 1 metrics: - id: 12 version: 1 - id: 12 version: 2","title":"How to define a context"},{"location":"basics/#task","text":"Description : Logical definition of Causal/Machine Learning tasks. Function : Handles configurations between dataset/model/metric components through python functions.","title":"Task"},{"location":"basics/#how-to-define-a-task","text":"A task takes in two files: a .yaml file that defines the type, name, path and the class name of a task, and a .py file that defines the task and its execution. The yaml file should only include the type , name , path , and the class fields. type: task name: discovery.static path: discovery.static.py class_name: DiscoveryStatic The python file should include any input and output functions for any dataset -> model, model -> metric, dataset -> metric relations, and the helper functions to facilitate the task. discovery.static.py and discovery.temporal.py files under the /task folder provides examples on construction of these functions.","title":"How to define a task"},{"location":"basics/#benchmark","text":"Description : Benchmark defines run results of a number of executed contexts. On top of the executed context structure, benchmark includes run-configuration specific information like the system configuration, GPU-CPU resource profiling.","title":"Benchmark"},{"location":"install/","text":"How to Install CausalBench Install the latest version of CausalBench package on pip: pip install causalbench-asu Prerequisites CausalBench package runs on Python 3.10+ and requires several packages and registration to the CausalBench in order to run. Package Prerequisites: CausalBench requires several packages in order to execute, these packages are downloaded alongisde CausalBench: gputil requests pyyaml bunch_py3 pandas jsonschema pipreqs psutil py-cpuinfo pip-system-certs pyadl gcastle Configuration CausalBench requires an user account registered in https://causalbench.org for authenticating with the system, downloading and uploading benchmark components. During execution, CausalBench creates a .causalbench folder in the user directory, which will host any downloaded components and user configuration. In order to authenticate with the system, you must create a config.yaml file with your username and password/access token in this format (without brackets): email: [your email address] password: [your access token/password]","title":"Installation"},{"location":"install/#how-to-install-causalbench","text":"Install the latest version of CausalBench package on pip: pip install causalbench-asu","title":"How to Install CausalBench"},{"location":"install/#prerequisites","text":"CausalBench package runs on Python 3.10+ and requires several packages and registration to the CausalBench in order to run.","title":"Prerequisites"},{"location":"install/#package-prerequisites","text":"CausalBench requires several packages in order to execute, these packages are downloaded alongisde CausalBench: gputil requests pyyaml bunch_py3 pandas jsonschema pipreqs psutil py-cpuinfo pip-system-certs pyadl gcastle","title":"Package Prerequisites:"},{"location":"install/#configuration","text":"CausalBench requires an user account registered in https://causalbench.org for authenticating with the system, downloading and uploading benchmark components. During execution, CausalBench creates a .causalbench folder in the user directory, which will host any downloaded components and user configuration. In order to authenticate with the system, you must create a config.yaml file with your username and password/access token in this format (without brackets): email: [your email address] password: [your access token/password]","title":"Configuration"},{"location":"quickstart/","text":"How to Run CausalBench You can use the python notebook attached, or follow along this page to get CausalBench into action. Install CausalBench, Please follow the installation guide for installing CausalBench. Install the latest version of CausalBench package on pip: pip install causalbench-asu Create a context context1: Context = Context.create(module_id=10, name='Context1', description='Test static context', task='discovery.static', datasets=[(dataset1, {'data': 'file1', 'ground_truth': 'file2'})], models=[model1, model2], metrics=[metric1, metric2]) Download a pre-set context Downloading a pre-set context (or any component) is straightforward, create a context object, and provide the module_id and version you'd like to download from the CausalBench context repository. context1: Context = Context(module_id=1, version=1) Upload a component You can publish any component with the publish() function. Publish function takes in the public boolean argument which is False by default. In case you wish to publish directly as public, set the argument as True . context1.publish(public=True) Run a benchmark With a set context, running a benchmark is done with execute() function. run: Run = context1.execute() This will execute the benchmark. You can also print the run using print(run) command. Upload results Uploading results works the same as uploading any component, by using the publish() function. run.publish(public=True) Example CausalBench Execution: This example will fetch a context, execute it, and upload the results. from causalbench.modules import Run from causalbench.modules.context import Context def main(): # Select and fetch the Context context1: Context = Context(module_id=1, version=1) # Run selected Context run: Run = context1.execute() # Print Run execution results print(run) # Publish the Run run.publish() if __name__ == '__main__': main()","title":"Quickstart"},{"location":"quickstart/#how-to-run-causalbench","text":"You can use the python notebook attached, or follow along this page to get CausalBench into action.","title":"How to Run CausalBench"},{"location":"quickstart/#install-causalbench","text":"Please follow the installation guide for installing CausalBench. Install the latest version of CausalBench package on pip: pip install causalbench-asu","title":"Install CausalBench,"},{"location":"quickstart/#create-a-context","text":"context1: Context = Context.create(module_id=10, name='Context1', description='Test static context', task='discovery.static', datasets=[(dataset1, {'data': 'file1', 'ground_truth': 'file2'})], models=[model1, model2], metrics=[metric1, metric2])","title":"Create a context"},{"location":"quickstart/#download-a-pre-set-context","text":"Downloading a pre-set context (or any component) is straightforward, create a context object, and provide the module_id and version you'd like to download from the CausalBench context repository. context1: Context = Context(module_id=1, version=1)","title":"Download a pre-set context"},{"location":"quickstart/#upload-a-component","text":"You can publish any component with the publish() function. Publish function takes in the public boolean argument which is False by default. In case you wish to publish directly as public, set the argument as True . context1.publish(public=True)","title":"Upload a component"},{"location":"quickstart/#run-a-benchmark","text":"With a set context, running a benchmark is done with execute() function. run: Run = context1.execute() This will execute the benchmark. You can also print the run using print(run) command.","title":"Run a benchmark"},{"location":"quickstart/#upload-results","text":"Uploading results works the same as uploading any component, by using the publish() function. run.publish(public=True)","title":"Upload results"},{"location":"quickstart/#example-causalbench-execution","text":"This example will fetch a context, execute it, and upload the results. from causalbench.modules import Run from causalbench.modules.context import Context def main(): # Select and fetch the Context context1: Context = Context(module_id=1, version=1) # Run selected Context run: Run = context1.execute() # Print Run execution results print(run) # Publish the Run run.publish() if __name__ == '__main__': main()","title":"Example CausalBench Execution:"},{"location":"terms/","text":"Terms CausalBench is an open-source tool for facilitating benchmarks and a public repository for datasets, models and metrics. It is at utmost importance to maintain a collaborative, supportive, and an open space for all ideas and research. By signing up, you agree to following: Usage rights: You are responsible for your use of CausalBench, with accordance of any applicable national and international laws. Your use of CausalBench will be limited to open source research, you cannot use any of our services for any illegal and unethical reasons, including taking actions that may harm CausalBench\u2019s collaborative environment. Submitting content: You may create and upload user generated content on CausalBench. By submitting content, you affirm that you have the necessary licenses, permissions and rights to sharing the content, and take responsibility for the content and any harm may be caused by it. Any content that you submit and make public in CausalBench will be made available for public use, and will be recorded permanently. You grant CausalBench the right to host and use the content, you grant the users the right to use the content for non-commercial research purposes. Simply put: Do not steal. Whether it\u2019s data, or citation; give credit wherever and whenever it\u2019s due. Make no harm. To the system, or to any of the users by any means. Understand research is a collaborative and open environment. Take responsibility in your work. Anything you make public will be recorded permanently. Failing to comply with these terms will result in appropriate action, including a suspension of the account.","title":"Terms"},{"location":"terms/#terms","text":"CausalBench is an open-source tool for facilitating benchmarks and a public repository for datasets, models and metrics. It is at utmost importance to maintain a collaborative, supportive, and an open space for all ideas and research. By signing up, you agree to following: Usage rights: You are responsible for your use of CausalBench, with accordance of any applicable national and international laws. Your use of CausalBench will be limited to open source research, you cannot use any of our services for any illegal and unethical reasons, including taking actions that may harm CausalBench\u2019s collaborative environment. Submitting content: You may create and upload user generated content on CausalBench. By submitting content, you affirm that you have the necessary licenses, permissions and rights to sharing the content, and take responsibility for the content and any harm may be caused by it. Any content that you submit and make public in CausalBench will be made available for public use, and will be recorded permanently. You grant CausalBench the right to host and use the content, you grant the users the right to use the content for non-commercial research purposes. Simply put: Do not steal. Whether it\u2019s data, or citation; give credit wherever and whenever it\u2019s due. Make no harm. To the system, or to any of the users by any means. Understand research is a collaborative and open environment. Take responsibility in your work. Anything you make public will be recorded permanently. Failing to comply with these terms will result in appropriate action, including a suspension of the account.","title":"Terms"}]}